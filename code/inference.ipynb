{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227be2c8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-18T02:21:56.481001Z",
     "iopub.status.busy": "2024-08-18T02:21:56.480683Z",
     "iopub.status.idle": "2024-08-18T02:25:14.466687Z",
     "shell.execute_reply": "2024-08-18T02:25:14.465347Z"
    },
    "papermill": {
     "duration": 197.993692,
     "end_time": "2024-08-18T02:25:14.469477",
     "exception": false,
     "start_time": "2024-08-18T02:21:56.475785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 24.6.1 requires cubinlinker, which is not installed.\r\n",
      "cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 24.6.1 requires ptxcompiler, which is not installed.\r\n",
      "cuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\r\n",
      "cudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\r\n",
      "jupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cp /kaggle/input/lmsys-utils/utils.py /kaggle/working/utils.py\n",
    "!pip install -q vllm==0.5.1 --no-index --find-links ../input/lib-vllm051\n",
    "!pip install -q transformers==4.42.4 --no-index --find-links /kaggle/input/lib-transformers4424\n",
    "!pip install -q peft==0.11.1 --no-index --find-links /kaggle/input/lib-peft0111\n",
    "!pip install -q bitsandbytes==0.43.1 --no-index --find-links /kaggle/input/lib-bitsandbytes0431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ef7e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T02:25:14.478800Z",
     "iopub.status.busy": "2024-08-18T02:25:14.478021Z",
     "iopub.status.idle": "2024-08-18T02:41:45.931059Z",
     "shell.execute_reply": "2024-08-18T02:41:45.929974Z"
    },
    "papermill": {
     "duration": 991.460854,
     "end_time": "2024-08-18T02:41:45.934297",
     "exception": false,
     "start_time": "2024-08-18T02:25:14.473443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 02:25:22.090371: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-18 02:25:22.090508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-18 02:25:22.240714: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__ = '2.3.0+cu121'\n",
      "transformers.__version__ = '4.42.4'\n",
      "peft.__version__ = '0.11.1'\n",
      "bitsandbytes.__version__ = '0.43.3'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "\n",
    "    LlamaPreTrainedModel,\n",
    "    LlamaModel,\n",
    "    LlamaForSequenceClassification,\n",
    "\n",
    "    Gemma2PreTrainedModel,\n",
    "    Gemma2Model,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "\n",
    "    PreTrainedTokenizerFast,\n",
    "    PreTrainedTokenizerBase, \n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "\n",
    "from utils import get_now_time_fullstring, seed_everything, current_date_time\n",
    "from utils import process, compute_metrics\n",
    "\n",
    "import torch\n",
    "print(f\"{torch.__version__ = }\")\n",
    "\n",
    "import transformers\n",
    "print(f\"{transformers.__version__ = }\")\n",
    "\n",
    "import peft\n",
    "print(f\"{peft.__version__ = }\")\n",
    "\n",
    "import bitsandbytes\n",
    "print(f\"{bitsandbytes.__version__ = }\")\n",
    "\n",
    "assert torch.cuda.device_count() == 2, \"Sorry - multi-GPU required!\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)  # Doesn't have any effect as Flash Attention does not support T4/P100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982a82eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T02:41:45.945252Z",
     "iopub.status.busy": "2024-08-18T02:41:45.944582Z",
     "iopub.status.idle": "2024-08-18T02:41:47.051712Z",
     "shell.execute_reply": "2024-08-18T02:41:47.050661Z"
    },
    "papermill": {
     "duration": 1.115822,
     "end_time": "2024-08-18T02:41:47.054152",
     "exception": false,
     "start_time": "2024-08-18T02:41:45.938330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_path = '/kaggle/input/gemma9b-ru'\n",
    "MAX_LEN = 2048+1024\n",
    "PADDING_SIDE = \"right\"\n",
    "TRUNCATION_SIDE = \"right\"\n",
    "\n",
    "use_softcapping = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "df['prompt'] = df['prompt'].apply(process)\n",
    "df['response_a'] = df['response_a'].apply(process)\n",
    "df['response_b'] = df['response_b'].apply(process)\n",
    "df['id'] = df['id'].astype(str)\n",
    "\n",
    "df_tta = df.copy()\n",
    "df_tta[\"response_a\"], df_tta[\"response_b\"] = df_tta[\"response_b\"], df_tta[\"response_a\"]\n",
    "\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(weights_path)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = PADDING_SIDE\n",
    "if TRUNCATION_SIDE == \"left\":\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    \n",
    "def tokenize_cls_p3(example, tokenizer, max_length, is_train):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    dot_tokens = tokenizer(\"......\", add_special_tokens=False)[\"input_ids\"]\n",
    "    final_p_tokens = tokenizer(\"\\n\\n---\\nWhich response is better? [A or B or tie]\\nAnswer: \", add_special_tokens=False)[\"input_ids\"]\n",
    "    for ps, ras, rbs in zip(example['prompt'], example['response_a'], example['response_b']):\n",
    "        one_input_ids = [tokenizer.bos_token_id]\n",
    "        prev_tokens_num = 2 + len(final_p_tokens) # 2 for bos_token and eos_token\n",
    "        for idx, (p, ra, rb) in enumerate(zip(ps, ras, rbs)):\n",
    "            r_tokens  = tokenizer(f'\\n\\n## Round {idx+1}:' if idx else f'## Round {idx+1}:', add_special_tokens=False)[\"input_ids\"]\n",
    "            p_tokens  = tokenizer(f'\\n### Prompt:\\n{p}', add_special_tokens=False)[\"input_ids\"]\n",
    "            ra_tokens = tokenizer(f'\\n\\n### Response A:\\n{ra}', add_special_tokens=False)[\"input_ids\"]\n",
    "            rb_tokens = tokenizer(f'\\n\\n### Response B:\\n{rb}', add_special_tokens=False)[\"input_ids\"]\n",
    "            all_tokens_num = prev_tokens_num + len(r_tokens) + len(p_tokens) + len(ra_tokens) + len(rb_tokens)\n",
    "\n",
    "            if all_tokens_num > max_length:\n",
    "                remain_tokens_num = max_length - prev_tokens_num - len(r_tokens) - 3*len(dot_tokens) \n",
    "                if remain_tokens_num >= 80:\n",
    "                    p_tokens  =  p_tokens[:int(remain_tokens_num*0.2)] + dot_tokens if len( p_tokens) > int(remain_tokens_num*0.2) else  p_tokens\n",
    "                    ra_tokens = ra_tokens[:int(remain_tokens_num*0.4)] + dot_tokens if len(ra_tokens) > int(remain_tokens_num*0.4) else ra_tokens\n",
    "                    rb_tokens = rb_tokens[:int(remain_tokens_num*0.4)] + dot_tokens if len(rb_tokens) > int(remain_tokens_num*0.4) else rb_tokens\n",
    "                    one_input_ids += r_tokens + p_tokens + ra_tokens + rb_tokens\n",
    "                break\n",
    "            else:\n",
    "                prev_tokens_num = all_tokens_num\n",
    "                one_input_ids += r_tokens + p_tokens + ra_tokens + rb_tokens\n",
    "        \n",
    "        one_input_ids += final_p_tokens + [tokenizer.eos_token_id]\n",
    "        one_attention_mask = [1] * len(one_input_ids)\n",
    "\n",
    "        input_ids.append(one_input_ids)\n",
    "        attention_mask.append(one_attention_mask)\n",
    "    \n",
    "    if is_train:\n",
    "        labels = [0 if a_win else 1 if b_win else 2 for a_win, b_win, tie in zip(example['winner_model_a'], example['winner_model_b'], example['winner_tie'])]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def infer_process_batch(df, tokenizer, max_length, batch_size):\n",
    "    results = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        example = {\n",
    "            'prompt': batch['prompt'].tolist(),\n",
    "            'response_a': batch['response_a'].tolist(),\n",
    "            'response_b': batch['response_b'].tolist(),\n",
    "        }\n",
    "        tokenized = tokenize_cls_p3(example, tokenizer, max_length, is_train=False)\n",
    "        results[\"input_ids\"].extend(tokenized[\"input_ids\"])\n",
    "        results[\"attention_mask\"].extend(tokenized[\"attention_mask\"])\n",
    "        \n",
    "    return results\n",
    "    \n",
    "    \n",
    "    \n",
    "tokenized_results = infer_process_batch(\n",
    "    df=df, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=MAX_LEN, \n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "tokenized_results_tta = infer_process_batch(\n",
    "    df=df_tta, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=MAX_LEN, \n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = df[\"id\"]\n",
    "data['input_ids'] = tokenized_results['input_ids']\n",
    "data['attention_mask'] = tokenized_results['attention_mask']\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "\n",
    "data_tta = pd.DataFrame()\n",
    "data_tta[\"id\"] = df_tta[\"id\"]\n",
    "data_tta['input_ids'] = tokenized_results_tta['input_ids']\n",
    "data_tta['attention_mask'] = tokenized_results_tta['attention_mask']\n",
    "data_tta[\"length\"] = data_tta[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9222ce2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T02:41:47.063816Z",
     "iopub.status.busy": "2024-08-18T02:41:47.063306Z",
     "iopub.status.idle": "2024-08-18T02:44:37.370614Z",
     "shell.execute_reply": "2024-08-18T02:44:37.369465Z"
    },
    "papermill": {
     "duration": 170.31601,
     "end_time": "2024-08-18T02:44:37.374363",
     "exception": false,
     "start_time": "2024-08-18T02:41:47.058353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b73406da39e4f70a524b8aa5ac05a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad6533be299414aa05d83e205bb5fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 0\n",
    "device_0 = torch.device('cuda:0')\n",
    "base_model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    weights_path,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model_0.config.use_cache = False\n",
    "\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "base_model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    weights_path,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model_1.config.use_cache = False\n",
    "\n",
    "if not use_softcapping:\n",
    "    base_model_0.config.attn_logit_softcapping = None\n",
    "    base_model_1.config.attn_logit_softcapping = None\n",
    "\n",
    "\n",
    "model_0 = base_model_0\n",
    "model_1 = base_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8223d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T02:44:37.385924Z",
     "iopub.status.busy": "2024-08-18T02:44:37.385627Z",
     "iopub.status.idle": "2024-08-18T02:44:37.402695Z",
     "shell.execute_reply": "2024-08-18T02:44:37.401839Z"
    },
    "papermill": {
     "duration": 0.025668,
     "end_time": "2024-08-18T02:44:37.404629",
     "exception": false,
     "start_time": "2024-08-18T02:44:37.378961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=2, max_length=MAX_LEN):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        \n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d8c274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T02:44:37.414188Z",
     "iopub.status.busy": "2024-08-18T02:44:37.413912Z",
     "iopub.status.idle": "2024-08-18T02:44:37.469615Z",
     "shell.execute_reply": "2024-08-18T02:44:37.468834Z"
    },
    "papermill": {
     "duration": 0.062984,
     "end_time": "2024-08-18T02:44:37.471730",
     "exception": false,
     "start_time": "2024-08-18T02:44:37.408746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data     = data.sort_values(\"length\", ascending=False)\n",
    "data_tta = data_tta.sort_values(\"length\", ascending=False)\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.copy()\n",
    "sub_2 = data_tta.copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "    \n",
    "results_list = list(results)\n",
    "result_df = results_list[0].sort_values(\"id\", ascending=True).reset_index(drop=True)\n",
    "result_df_tta = results_list[1].sort_values(\"id\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "proba_tta = result_df_tta[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.loc[:, \"winner_model_a\"] = (proba[:, 0] + proba_tta[:, 1]) / 2\n",
    "submission_df.loc[:, \"winner_model_b\"] = (proba[:, 1] + proba_tta[:, 0]) / 2\n",
    "submission_df.loc[:, \"winner_tie\"]     = (proba[:, 2] + proba_tta[:, 2]) / 2\n",
    "submission_df['id'] = submission_df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78772d76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T02:44:37.481704Z",
     "iopub.status.busy": "2024-08-18T02:44:37.481414Z",
     "iopub.status.idle": "2024-08-18T02:44:38.829010Z",
     "shell.execute_reply": "2024-08-18T02:44:38.827935Z"
    },
    "papermill": {
     "duration": 1.355512,
     "end_time": "2024-08-18T02:44:38.831522",
     "exception": false,
     "start_time": "2024-08-18T02:44:37.476010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "df2['id'] = df2['id'].astype(str)\n",
    "\n",
    "a_null_df = df2[(df2[\"response_a\"]== '[null]') | (df2[\"response_a\"]== '[]') | (df2[\"response_a\"]== '[ ]') | (df2[\"response_a\"]== '[  ]') | (df2[\"response_a\"]== '[\"\"]') | (df2[\"response_a\"]== '[\"\",\"\"]')]\n",
    "a_null_id_list = a_null_df[\"id\"].tolist()\n",
    "submission_df.loc[submission_df['id'].isin(a_null_id_list), ['winner_model_a', 'winner_model_b', 'winner_tie']] = [0.04, 0.88, 0.08]\n",
    "\n",
    "\n",
    "b_null_df = df2[(df2[\"response_b\"]== '[null]') | (df2[\"response_b\"]== '[]') | (df2[\"response_b\"]== '[ ]') | (df2[\"response_b\"]== '[  ]') | (df2[\"response_b\"]== '[\"\"]') | (df2[\"response_b\"]== '[\"\",\"\"]')]\n",
    "b_null_id_list = b_null_df[\"id\"].tolist()\n",
    "submission_df.loc[submission_df['id'].isin(b_null_id_list), ['winner_model_a', 'winner_model_b', 'winner_tie']] = [0.88, 0.04, 0.08]\n",
    "\n",
    "\n",
    "same_a_b_df2 = df2[(df2[\"response_a\"]==df2[\"response_b\"])]\n",
    "same_a_b_id_list = same_a_b_df2[\"id\"].tolist()\n",
    "submission_df.loc[submission_df['id'].isin(same_a_b_id_list), ['winner_model_a', 'winner_model_b', 'winner_tie']] = [0.06, 0.06, 0.88]\n",
    "\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "!rm -rf __pycache__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb2ee0",
   "metadata": {
    "papermill": {
     "duration": 0.004561,
     "end_time": "2024-08-18T02:44:38.840817",
     "exception": false,
     "start_time": "2024-08-18T02:44:38.836256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5483079,
     "sourceId": 9087141,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5483552,
     "sourceId": 9087780,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5490761,
     "sourceId": 9098110,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5483473,
     "sourceId": 9108175,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5556381,
     "sourceId": 9191370,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5556153,
     "sourceId": 9194175,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 192966652,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 192966691,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 192966761,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 192966821,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1368.884669,
   "end_time": "2024-08-18T02:44:42.396775",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-18T02:21:53.512106",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01515fd1ef884674b405bf26223e4031": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0ced35ac51454293ac2140a4eefa3baf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0edacaf979944ca5a29d1ad7d5a806e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0f4948354aad44aba1481e495b5fcd93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "27ac383a04e14eefab77f1e0abc09320": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2b73406da39e4f70a524b8aa5ac05a39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_669355a842df4a1991a6cd864fcd0435",
        "IPY_MODEL_51d39624c18f49f5930cd38d4670a9cf",
        "IPY_MODEL_bc69669d038845d1bfa5b4eb571fa022"
       ],
       "layout": "IPY_MODEL_0f4948354aad44aba1481e495b5fcd93"
      }
     },
     "4a6597e2077343a89931d99747500f63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "51d39624c18f49f5930cd38d4670a9cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e8100ae6d41d4ecb85f16c6319401a40",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_01515fd1ef884674b405bf26223e4031",
       "value": 4.0
      }
     },
     "669355a842df4a1991a6cd864fcd0435": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d23b104e744d4fc983d49612d7f01018",
       "placeholder": "​",
       "style": "IPY_MODEL_0edacaf979944ca5a29d1ad7d5a806e2",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "7767f069a8f046edb45f15101055b24d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bc30e500a704407d97bd17cb27aa6335",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0ced35ac51454293ac2140a4eefa3baf",
       "value": 4.0
      }
     },
     "7cc83f0f4fc541bb80e55afe3016bfc4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f3f715f224f7481eb31665b7b40dd085",
       "placeholder": "​",
       "style": "IPY_MODEL_ebe211378bbe43ffb217ace4cd419160",
       "value": " 4/4 [01:25&lt;00:00, 20.51s/it]"
      }
     },
     "879d95bcd3254aa5bfce40a9fd11ce77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aad6533be299414aa05d83e205bb5fe0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f0083d76b38e434baca1ae602a9c777e",
        "IPY_MODEL_7767f069a8f046edb45f15101055b24d",
        "IPY_MODEL_7cc83f0f4fc541bb80e55afe3016bfc4"
       ],
       "layout": "IPY_MODEL_4a6597e2077343a89931d99747500f63"
      }
     },
     "bc30e500a704407d97bd17cb27aa6335": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc69669d038845d1bfa5b4eb571fa022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_879d95bcd3254aa5bfce40a9fd11ce77",
       "placeholder": "​",
       "style": "IPY_MODEL_27ac383a04e14eefab77f1e0abc09320",
       "value": " 4/4 [01:24&lt;00:00, 20.19s/it]"
      }
     },
     "d23b104e744d4fc983d49612d7f01018": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d4d8513bfea04f57bba697840d428947": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8100ae6d41d4ecb85f16c6319401a40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ebe211378bbe43ffb217ace4cd419160": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f0083d76b38e434baca1ae602a9c777e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d4d8513bfea04f57bba697840d428947",
       "placeholder": "​",
       "style": "IPY_MODEL_fe8ff892f62149d5974a9c05fe5c6b0d",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "f3f715f224f7481eb31665b7b40dd085": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fe8ff892f62149d5974a9c05fe5c6b0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
